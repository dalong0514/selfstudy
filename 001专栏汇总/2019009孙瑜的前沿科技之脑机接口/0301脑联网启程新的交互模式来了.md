# 0301. 脑联网启程：新的交互模式来了
> 前沿科技之脑机接口
2019-04-30

就在刚刚，脑机接口又取得了一次重大的突破，4 月 24 日，《自然》杂志（Nature）发表了这项研究，你不用开口，机器就能听懂你想说什么，帮你说出口。全美媒体争相报道，我希望通过这篇加餐赶快和你分享这一激动人心的成果。

华裔科学家 Edward Chang（爱德华-张）教授和他的同伴，实现了脑电和语言的直接转换。也就是说，你连上脑机接口之后，就会像霍金一样，机器可以帮你说出你的想法。我认为，这个研究可以说是脑联网启动的第一步。

点击文稿，我给你放了一个视频，它清晰地展示了研究的过程。

不过，课程里也讲到过，科学家能解密大脑的语言，为什么这项研究引起了这么大的热议呢？因为，直到这项研究出来，我们才意识到，新的交互模式真的出现了。

那为什么马斯克的 Neuralink，还有扎克伯格的 Facebook 都投入了巨大的人力财力呢？就是想要引领下一代的交互模式。

过去，语言是人类交互模式最大的突破，你去 5 万年前拉一个人穿越到今天，你会发现衣食住行全变了，只有交流方式还是一样的。

那下一代交互模式的突破是什么？像三体人一样脑脑交互吗？

在这项成果发表之前，关于脑机沟通最主要的研究就是脑控打字。不过脑控打字只能算是退而求其次的方案，因为这可不是你想说什么，机器就打出来了。这个真实的过程特别「非人类」。

脑控打字

为什么呢？我来简单说说，脑控打字至少要分成 3 步：

1. 采集并翻译脑电信号；

2. 转换成指令；

3. 打字。

你发现了吗？这里面有一步至关重要——转换成指令。这是什么意思呢？

如果你记得巴西男孩操控机械战甲就会想起来，「指令」其实靠的是被试者的「运动想象」去控制的。也就是你想完成的任务是，让电脑光标向上移动，但你这时候想的却是「左腿踢腿」。这个和我们大脑的正常活动差太多了。

Facebook 在这个领域的研究很前沿，2017 年 4 月的时候，他们的前沿科技研发部门就透露了脑控打字的研究。当时可以做到每分钟打 8 个单词，已经是非常惊人的突破了。这就相当于如果用智能手机打字，你打 3 个字的时候，瘫痪患者用意念也能打出 1 个字。

不过，脑控打字目前还没有取得巨大的突破，Edward Chang（爱德华-张）教授的这项研究却已经完成了从意念直接到语言的转换，而且速度还翻了十几倍，语速可达到了每分钟 150 个单词，逼近了普通人的语速。为什么一下子语速能够有这么大的提升，他们是怎么做到的呢？

你要知道，大脑信号是没办法直接捕捉的，因为我们压根没有解密大脑中「想法」的脑电信号是什么。怎么办呢？

Edward Chang（爱德华-张）教授和同伴找了一个很聪明的办法。在训练的时候，研究人员会请受试者大声阅读大量的句子，比如说读一些小孩子的故事书，像《睡美人》《爱丽丝梦游仙境》这些经典小人书，然后他们把脑电信号和语言的转化工作分成两步。

第一步，解锁发声动作。一个人说话的时候，他的下巴、喉咙、嘴唇和舌头的运动和发出的声音有很清晰的对应关系。比如你张大口，喉咙发出声音，可能就在说「啊」。

第二步，他们成功解码了脑信号与发生动作的关系。同样是运用人工智能的算法，爱德华·张（Edward Chang）教授发现了哪一组信号对应哪个器官运动的组合。

你发现了吗？有了发声动作这个中介，大脑中神秘的想法可以被清晰捕捉了。

另一个关键的技术就是人工智能算法，也就是循环神经网络（recursive neural network, RNN），像我们每天都在微信上使用的语音转文字的功能，就是用了循环神经网络，因为这种算法在处理语言上有巨大的优势。当数据有复杂时间结构的时候，循环神经网络绝对是个利器。这个算法可以解码声道咬合关节运动，把这些运动转化成句子。

![](https://raw.githubusercontent.com/dalong0514/selfstudy/master/图片链接/生命科学/2019115.jpg)

这项成果的确激动人心，但是不是说大脑与机器真的能无障碍沟通了呢？脑机接口的任务是不是已经完成了呢？

其实远远还没有，要使用这个系统算法模型，做出真正临床语音合成脑机接口，还有太多挑战。

比如说，信号采集的困难。这项研究用的是脑皮层电极（EcoG），这种电极只是覆盖在大脑表面，没有完全插到脑组织里面去。所以采集的信号相对来说会弱一些，还不能达到对单个神经元的一对一采集，分辨率是有限的。

![](https://raw.githubusercontent.com/dalong0514/selfstudy/master/图片链接/生命科学/2019116.jpg)

ECoG 电极阵列由记录大脑活动的颅内电极组成

第二呢，现在数据的来源都是从一些会说话的病人采集的，那这样一个训练出来的算法，可不可以用到那些由于中风和渐冻人症（ALS）导致失语的人身上呢？毕竟，我们是通过语音训练出来的，而中风病人和渐冻人症（ALS）患者，他们的下巴和舌头、嘴唇或者是喉咙可能都是动不了的。

第三，因为中风在使人丧失说话能力的同时，往往也会损害甚至摧毁支持语言表达的脑区，所以如果我们在中风病人上面采集数据，可能就采不到了。为什么呢？因为他们的那一片脑区已经死亡，不会放电了，所以我们也就采集不到数据了。

不过顺着这样的思路，未来「语言障碍」会成为历史，每个人都能获得自由表达思想的能力，即使是那些瘫痪、中风、闭锁症的患者都可以和周遭世界重新建立连接。

再进一步呢，既然机器能直接听懂你想说什么，并且还能帮你传递给别人，那虽然脑脑交互暂时没有实现，但人们运用语言的场景也可能有翻天覆地的变化。

比如说下次你开会的时候，偷偷戴个耳机，可能就不是在听音乐，而是和一个人打电话，或者只是在和隔壁的同事聊天。虽然你俩都没有开口，但可能透过脑机接口已经聊得如火如荼了。

当然了，这一切并不容易，但我很乐观地相信，这一天应该不远。

最后说一下，最近马斯克在 Twitter 上发文，说他会在几个月内宣布一些 Neuralink 的最新进展，我会为你时刻关注。如果有相关讯息，我会第一时间向你汇报。

参考文献：

Nature volume 568, pages 493-498(2019)

视频来源：

UCSF Neurosurgery

https://www.youtube.com/watch?v=kbX9FLJ6WKw