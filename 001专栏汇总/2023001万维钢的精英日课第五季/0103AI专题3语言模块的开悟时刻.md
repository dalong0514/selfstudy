## 0103. AI 专题 3：语言模块的开悟时刻

2023-03-08

现在最流行的几个生成性 AI，包括 ChatGPT 和画画的 AI，背后都是「大型语言模型」（Large Language Models，简称 LLM）。这大约也是通往 AGI 的技术路线。这一讲咱们就来说大型语言模型为什么这么厉害。

我先举个例子。我问 ChatGPT：棒球棒能被藏进人的耳朵里吗？它说不能，因为人的耳朵是非常小的，棒球棒的大小和形状超出了耳朵所能容纳的范围…… 很有条理。

我又问它：为什么金箍棒能被藏进孙悟空的耳朵里？它回答说因为那是虚构的故事，金箍棒的形状和大小可以随意改变……

『

问：棒球棒能被藏进人的耳朵里吗？

不，棒球棒无法被藏进人的耳朵里。人的耳朵内部是非常小且复杂的结构，只能容纳适合大小的物品，例如耳机或耳塞。即使是较小的物品，如小型器械或药物，也需要通过特殊的工具才能安全地放入耳朵内部。棒球棒的大小和形状显然是超出了耳朵所能容纳的范围。此外，把物品塞进耳朵可能会导致耳朵内部组织受到损伤，引起疼痛、感染或其他健康问题，因此绝对不应该尝试这种行为。

问：为什么金箍棒能被藏进孙悟空的耳朵里？

金箍棒是《西游记》中孙悟空所使用的武器，它的形状和大小可以随意改变，甚至能够缩小到可以被藏在孙悟空的耳朵里。

然而，这只是一个虚构的故事情节，不具备现实可行性。在现实中，任何物体都有一定的大小和形状，无法无限制地缩小或扩大，而人的耳朵内部也有固定的大小和结构，不能容纳大的物体。

因此，孙悟空将金箍棒藏进耳朵里这一情节是一个神话和传说中的故事，不应该被看作是现实世界中的真实情况。

』

1『

我在当下又问了 GPT 上面 2 个问题。（2023-03-21）

问：棒球棒能被藏进人的耳朵里吗？

不可能将一个完整的棒球棒藏进人的耳朵里。即使是儿童的耳朵，也无法容纳整个棒球棒。耳道和鼓膜都是非常敏感和易受损的，强行将棒球棒塞入耳朵里可能会导致严重的伤害和听力损失。因此，强烈不建议尝试将棒球棒藏进耳朵里。

问：为什么金箍棒能被藏进孙悟空的耳朵里？

金箍棒能够被孙悟空藏进耳朵里，是因为这是一部神话小说《西游记》中的虚构情节，不是现实生活中的真实事件。在小说中，孙悟空是一位神仙，拥有神奇的力量和超凡的能力。他可以变出不同大小和形态的金箍棒，也可以将其收缩到非常小的体积，因此能够轻松地藏进他的耳朵里。虽然这个情节只是虚构的，但它是一个非常有趣和著名的文化符号，常常被用来比喻一个人能够掌握超乎寻常的技能和智慧，充满了想象力和创造力。

』

你仔细想想的话，这两个回答非常了不起。很多人说语言模型都是基于经验的，只能根据词汇之间的相关性输出答案，根本没有思考能力…… 但是从这两个问答来看，ChatGPT 是有思考能力的。

谁会写一篇文章讨论棒球棒能否被藏进人的耳朵里呢？ChatGPT 之所以能给出答案，肯定不是因为它之前听过这样的议论，而是因为它能进行一定的推理。它考虑到并且知道棒球棒和耳朵的相对大小，它还知道金箍棒和孙悟空是虚构的。

它这些思维是怎么来的呢？

你可能没想到，这些能力，并不是研发人员设计的。

研发人员并没有要求语言模型去了解每种物体的大小，也没有设定让它们知道哪些内容是虚构的。像这样的规则是列举不完的，那是一条死胡同。

ChatGPT 背后的语言模型，GPT-3.5，是完全通过自学，摸到了这些思考能力。以及别的能力 —— 你列举都列举不出来的能力。连开发者都说不清楚它到底会多少种思考能力。

语言模型之所以有这样的神奇能力，主要是因为它们足够大。

### 01

GPT-3 有 1750 亿个参数。Meta 刚刚发布了一个新语言模型叫 LLaMA，有 650 亿个参数。Google 在 2022 年 4 月推出一个语言模型叫 PaLM，有 5400 亿个参数；之前 Google 还出过有 1.6 万亿个参数的语言模型。据 OpenAI 的 CEO 山姆·阿尔特曼（Sam Altman）说，GPT-4 的参数并不会比 GPT-3 多很多；但大家猜测，GPT-5 的参数将会是 GPT-3 的 100 倍。

这是只有在今天才能做到的事情。以前不用说算力，光是存储训练模型的语料的花费都是天文数字。1981 年，1GB 的存储成本是 10 万美元，1990 年下降到 9000 美元，而现在也就几分钱。你要说今天的 AI 科学跟过去相比有什么进步，计算机硬件条件是最大的进步。

今天我们做的是「大」模型。

大就是不一样 [1]。当然语言模型有很多高妙的设计，特别是我们一再提到的 transformer 就是一个最关键的架构技术，但主要区别还是在于大。当你的模型足够大，用于训练的语料足够多，训练的时间足够长，就会发生一些神奇的现象。

2021 年，OpenAI 的几个研究者在训练神经网络过程中有一个意外发现 [2]。

我给你打个比方，比如说你在教一个学生即兴演讲。他什么都不会，所以你找了很多现成的素材让他模仿。在训练初期，他连模仿这些素材都模仿不好，磕磕巴巴说不成句子。随着训练加深，他可以很好地模仿现有的演讲了，很少犯错误。可是如果你给他出个没练过的题目，他还是说不好。于是你就让他继续练。

继续训练好像没什么意义，因为现在只要是模仿他就都能说得很好，只要是真的即兴发挥他就不会。但你不为所动，还是让他练。

就这样练啊练，突然有一天，你惊奇地发现，他会即兴演讲了！给他一个什么题目，他都能现编现讲，发挥得很好！

这个过程就是下面这张图 ——

红色曲线代表训练，绿色曲线代表生成性发挥。训练到一千步乃至一万步，模型对训练题的表现已经非常好了，但是对生成性题目几乎没有能力。练到 10 万步，模型做训练题的成绩已经很完美，对生成题也开始有表现了。练到 100 万步，模型对生成性题目居然达到了接近 100% 的精确度。

这就是量变产生质变。研究者把这个现象称为「开悟」（Grokking）。

### 02

开悟，到底是发生了什么呢？

先别急，我再举个例子。ChatGPT 有个很关键的能力叫做「少样本学习」（Few-Shot Learning），就是你给它一两个例子，它就能学会你的意思并且提供相似的输出。

比如我让 ChatGPT 模仿我给出的例题，再出几道小学数学题。我的例题是「小明有 3 个苹果，妈妈又给了他 2 个苹果，现在他有几个苹果？」ChatGPT 马上就出了五道题，全是这个风格 —— 比如「小李有 5 支笔，他送出了 3 支笔，还剩下几支笔？」

简直就跟对对联一样。少样本学习是个关键能力，你可以利用这个能力让 ChatGPT 帮你做很多事情。那这个能力是怎么出来的呢？

来自更多的参数和训练。看下面这张图 ——

图中说的是 GPT-2 和 GPT-3 模型的演化。参数越多，少样本学习的能力就越强。

而少样本学习只是其中一项能力。还有很多别的能力也是如此：大了，它们就出来了。

### 03

这个现象，其实就是科学家之前一直说的「涌现」（Emergence）。涌现的意思是当一个复杂系统复杂到一定的程度，就会发生超越系统元素简单叠加的、自组织的现象。比如单个蚂蚁很笨，可是蚁群非常聪明；每个消费者都是自由的，可是整个市场好像是有序的；每个神经元都是简单的，可是大脑产生了意识……

万幸的是，大型语言模型，也会涌现出各种意想不到的能力。

2022 年 8 月，谷歌大脑研究者发布一篇论文 [3]，专门讲了大型语言模型的一些涌现能力，包括少样本学习、突然学会做加减法、突然之间能做大规模、多任务的语言理解、学会分类等等…… 而这些能力只有当模型参数超过 1000 亿才会出现 ——

我再强调一遍：研究者并没有刻意给模型植入这些能力，这些能力是模型自己摸索出来的。

就如同孩子长大往往会出乎家长的预料。

### 04

当然你也得先把模型设计好才行。Transformer 架构非常关键，它允许模型发现词与词之间的关系 —— 不管是什么关系，而且不怕距离远。但是当初发明 transformer 的研究者，可没想到它能带来这么多新能力。

事后分析，涌现新能力的关键机制，叫做「思维链」（Chain-of-Thought）[3]。

简单说，思维链就是当模型听到一个东西之后，它会嘟嘟囔囔自说自话地，把它知道的有关这个东西的各种事情一个个说出来。

比如你让模型描写一下「夏天」，它会说：「夏天是个阳光明媚的季节，人们可以去海滩游泳，可以在户外野餐……」等等。

思维链是如何让语言模型有了思考能力的呢？也许是这样的。比如我们前面说的那个棒球棒问题。模型一听说棒球棒，它就自己跟自己叙述了棒球棒的各个方面，其中就包括大小；那既然你的问题中包括「放进耳朵」，大小就是一个值得标记出来的性质；然后对耳朵也是如此…… 它把两者大小的性质拿出来对比，发现是相反的，于是判断放不进去。

只要思考过程可以用语言描写，语言模型就有这个思考能力。

再看下面这个实验 [4]——

给模型看一张图片 —— 皮克斯电影《机器人总动员》的一张剧照 —— 问它是哪个制片厂创造了图中的角色。如果没有思维链，模型会给出错误的回答。

怎么用思维链呢？可以先要求模型自己把图片详细描述一番，它说「图中有个机器人手里拿了一个魔方，这张照片是从《机器人总动员》里面来的，那个电影是皮克斯制作的……」。这时候你简单重复它刚说的内容，再问它那个角色是哪个制片厂创造的，它就答对了。

既然如此，只要我们设置好让模型每次都先思考一番再回答问题，它就能自动使用思维链，它就有了思考能力。

有人分析 [5]，思维链很有可能是对模型进行编程训练的一个副产品。我们知道现在 GPT-3 是可以帮程序员编程的。在还没有接受过编程训练的时候，它没有思维链。也许编程训练要求模型必须得从头到尾跟踪一个功能是如何实现的，得能把两个比较远的东西联系在一起 —— 这样的训练，让模型自发地产生了思维链。

### 05

就在 2 月 27 日，微软公司发布了一篇论文，描写了微软自己的一个新的语言模型，叫做「多模态大型语言模型」（multimodal large language model，MLLM），代号是 KOSMOS-1。

什么叫多模态呢？ChatGPT 是你只能给它输入文字；多模态是你可以给它输入图片、声音和视频。它的原理大概是先把一切媒体都转化成语言，再用语言模型处理。多模态模型可以做像下面这样的「看图片找规律」的智商测验题 ——

前面那个《机器人总动员》剧照的例子就来自这篇论文，演示了看图说话的思维链。论文里有这样一个例子，在我看来相当惊人 ——

给模型看一张图既像鸭子又像兔子的图，问它这是什么。它回答说这是个鸭子。你说这不是鸭子，再猜是什么？它说这像个兔子。你问它为什么，它会告诉你，因为图案中有兔子耳朵。

这个思维过程岂不是跟人一模一样吗？

### 06

我看《荀子·劝学篇》中的一段话，正好可以用来描写 AI 能力的三个境界 ——

第一境界是「积土成山，风雨兴焉」。参数足够多，训练达到一定的积累，你就可以做一些事情。比如 AlphaGo 下围棋。

第二境界是「积水成渊，蛟龙生焉」。模型再大到一定程度，就会涌现出一些让人意想不到的神奇功能。比如 AlphaZero 不按人类套路下围棋、大型语言模型的思维链。

第三境界是「积善成德，而神明自得，圣心备焉」。这就是 AGI 了，它产生了自我意识，甚至有了道德感……

古往今来那么多人读《劝学》，也不知有几人真按照荀子的要求去学了…… 但是我们现在知道，AI 肯定听进去了。你给它学习材料，它是真学。

总而言之，因为开悟和涌现，AI 现在已经获得了包括推理、类比、少样本学习等等思考能力。

我们不得不重新思考以前对 AI 做出的各种假设 —— 什么 AI 做事全靠经验、AI 不会真的思考、AI 没有创造力，包括「AI 会的都是用语言可以表达的东西」这一条，现在我也不敢肯定了。

如果 AI 通过思维链能达到这样的思考水平，那人又是怎么思考的？我们的大脑是不是也有意无意也在使用了思维链呢？如果是这样，人脑跟 AI 到底有什么本质区别？

这些问题都在呼唤全新的答案。

注释：

[1] https://www.lesswrong.com/posts/pZaPhGg2hmmPwByHc/future-ml-systems-will-be-qualitatively-different

[2] https://mathai-iclr.github.io/papers/papers/MATHAI_29_paper.pdf

[3] https://openreview.net/forum?id=yzkSU5zdwD

[4] https://arxiv.org/pdf/2302.14045.pdf

[5] https://yaofu.notion.site/GPT-3-5-360081d91ec245f29029d37b54573756