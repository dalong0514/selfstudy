## 0501. ChatGPT 的回答从哪里来？

我们在前面讲到，ChatGPT 回答问题，通常答案都比问题来得长，需要补上额外的信息，这件事是怎么做到的呢？我们这一讲就来谈谈这个问题。

在英语中，问题通常都是用一个疑问词开头，而所有的疑问词都由 W 和 H 开头，它们也被称为 WH 单词，包括「是什么」（What），「为什么」（Why），「是谁」（Who），「在什么时候」（When），「在哪里」（Where），「哪一个」（Which），以及「怎么做」（How）。

### 5.1 回答简单问题

前面提到，对计算机来说，除了「为什么」和「怎么做」的问题属于复杂问题，其他问题都属于简单问题。

比如现在 2023 年，你问 ChatGPT 美国总统是谁，它会很快告诉你是拜登。这是有明确答案的。这类问题，计算机根据疑问词和主题词就能理解了，比如这个问题的主题词是「美国总统」，疑问词是「是谁」，再去统计网上关于这个问题的答案，我们就可以建立一个关于美国总统的语义框架，或者说知识框架。

利用语言模型回答问题，不是一个问题对一个答案这样简单的匹配，而是对于问题给出多个答案，然后根据答案的概率排序，返回一个最可能的答案。比如过去的媒体上还有「美国总统特朗普」「美国总统布什」这样的信息，它们也会被统计进去。但是今天的语言模型都很聪明，它们会给最近的内容赋予较高的权重。最后你得到的答案，就会是当前美国总统的名字。当然，如果正好赶在换届时，数据还没有更新，给出的答案就可能出错。

其实都不一定是 ChatGPT，今天用苹果手机的 Siri 功能，或者 Google 的 Google Voice 功能，几乎可以回答所有简单的问题，基本上就是按照这种方法去做的。

讲到苹果的 Siri，很多人觉得它比 ChatGPT 笨多了，即便是很多关于事实的问题，它也给不出答案，直接说不知道。其实这是对美国的商业和法律缺乏了解。苹果作为大的上市公司，是不能随便给别人建议的，否则法律上的麻烦很多。此外，作为一个用户每天依赖的产品，可靠性非常重要，不能时对时错。因此，像 Siri 这类的产品，做不到就不做了。

### 5.2 回答复杂问题

说完简单问题，接下来我们说说复杂问题的回答。对于这类问题，计算机的做法和人有很大的不同。

人遇到这种问题时，有三种途径能够回答。

第一，你知道答案，直接给出。这种做法，计算机也采用。

比如答案就在某个问答网站内。过去计算机采用网页搜索，把那个网页提供出来。今天 ChatGPT 则是把相应的一段话抽取出来。

为什么今天和过去采用的做法不同呢？原因有两个。一个是过去计算机对文本进行摘要的能力不够，二是过去的搜索其实很少分析句子的语义，不确定用户的问题和问答网站上的问题是否一致。

比如你问「为什么最近苹果的股票不涨」，网页给你一个三个月前的网页，分析「为什么苹果的股票在涨」，因为这里面主要的关键词都能匹配上。你遇到这种情况会哭笑不得，觉得搜索质量很差。今天 ChatGPT 足够强大，大部分时候不会犯这样明显的错误。

我们回答复杂问题的第二种情况是，你不知道答案，但是你懂得找到答案的基本知识，于是你利用你的基本知识推出了答案。

比如当有人问你天为什么是蓝色的，你学了中学物理，知道太阳光是由七色光构成的，不同颜色的光折射率不同。同时你还能活学活用，想到阳光进入大气层时因为折射率不同会发生散射，导致天空显得是蓝色的。

但是，计算机其实没有这个能力。硅谷的几家著名公司（这里我就不点出名字了），对 ChatGPT 进行了全面的测试，发现它回答小学常识课（美国叫自然课）的问题，正确率还不到 60%。为什么呢？因为这部分问题很少在网络上被讨论，或者网络上没有靠谱的答案，而 ChatGPT 没有办法像人那样运用知识去寻找答案，只能从现成的答案里归纳总结。

人类解决复杂问题的第三种情况是，你不知道答案，而现有的知识也无法直接推导出答案，需要你做研究工作。

比如这几年疫情，市面上能找到几十种口罩，有些管用，有些不管用，有些虽然管用但是效果有限，到底哪种才靠谱？虽然你能够在互联网上找到一些这方面的内容，其实一大半都是过时的，甚至是想当然的、错误的。过去从来没有人对所有的口罩进行对比研究。这个看似并不复杂的问题，其实并没有好的答案。

2020 年，斯坦福大学材料学专家崔屹教授的团队进行了全面的研究，彻底回答了这个问题。比如他们发现，真正能够有效过滤新冠病毒的口罩，是靠口罩纤维上的静电力把病毒吸附在口罩上，让它无法进入口鼻，而不是用纤维网把病毒挡住。他们还发现，纤维越细的口罩，防护效果越好，纤维粗的反而防护效果差，这和人们的直观感觉有很大的差异。后来诺贝尔奖得主朱棣文教授给了一个合理的解释，就是静电场和纤维直径的平方成反比，纤维粗的口罩的吸附能力弱。这项研究还有很多发现，比如口罩一旦沾染了潮气，防护力基本上就丧失了。基于这项研究的综述论文发表之后，得到了广泛引用。

你看，对于这样的问题，人类是需要通过实验和探索发现新知，而且更正过去认识错误的。这显然也是 ChatGPT 做不到的。

### 5.3 ChatGPT 如何工作

那么 ChatGPT 的答案从哪里来呢？简而言之，回答问题也好、写作短文也好，都基于它对现有事实的抽取和整合，或者说归纳总结。

2012 年，我们在 Google 尝试计算机自动问答时，将当时互联网上全部高质量的英语句子，大约有 1000 亿个，全部做了语法分析。这是一件非常消耗算力的事情，计算量大约是对这些句子做简单统计的上亿倍。完成这项艰巨的任务之后，可以得到两类知识，一个是每一篇文章内自己所包含的知识，另一个则是全网的知识图谱。

在此之前，Google 收购了一家专门构建知识图谱的小公司，那家小公司已经完成了对上百万个知识点的总结，并且构建出它们之间的相互关系。在对上万亿语句进行分析后，这个知识图谱得到了很大的补充。这里必须要提一句，这中间的工作依然需要人工干预，无法全部靠机器完成。

后来，负责 Google 知识图谱的副总裁到了苹果，是今天苹果 Siri 的负责人。苹果的 Siri 其实背后也有一个知识图谱。ChatGPT 背后也一样，虽然它没有公布如何利用各种知识，但是它下载了整个维基百科，而维基百科有一个现成的知识图谱。

当人们在问答系统中提一个问题后，问答系统会首先在知识图谱中寻找可能答案。对于我们前面讲到的简单问题，只要问题靠谱，答案通常都能直接找到。

但是对于复杂问题，有些可以通过知识图谱中的几个相关联的知识点回答，但大部分就不行了，于是需要回到提供知识的原始网页中去寻找答案。这个过程其实是一个逆向思维的过程，和人的思维方式不太一样。

比如我现在问一个复杂问题，计算机就会去找到几十上百个可能包含答案的文章，然后从这些文章中提取一些语句，构成可能的答案。当然，为了让这些句子凑在一起看上去像是一个有逻辑的、连贯的回答，就需要使用语言模型了。

换句话说，这些候选的文章里有了原材料，而语言模型是一个厨师，将它做成一道菜。如果语言模型质量不高，提供的答案就是几个事实的堆砌，让人看了觉得毫无连贯性可言。ChatGPT 之所以比之前的问答系统做得好，并不是它的原材料更多，而是菜做得更精致。

了解了各种问答系统是如何回答问题的，我们再来看看它们如何实现对话与写作。

对话有点像句子中的填空游戏。比如有这样一句话：从去年（ ）开始，考研成了大学生最关注的话题之一。请问中间该填什么词？

对此，语言模型可以给出概率最高的几个词作为候选。比如上半年，下半年，夏天等等，不太可能提示「箱子」、「北京」、「土豆」这些不相关的词。类似地，如果一段话中拿走了一两个句子，今天语言模型也能填回去，只要语言模型足够大、足够好，填进去的内容读起来就通顺。

如果我们从句子层面扩展到段落层面，你说一句话，它说一句话，就是聊天了。它和一段文字中，抽走了一半的话，再用语言模型复原这段话的原理是一样的。由于聊天时你一句我一句，对话是发散的，并不需要下一句话一定是事先想到的，只要前后顺畅，聊天就能进行下去。如果真遇到它填不出来的的句子，它通常会打哈哈，让你提供更多的输入，直到聊天能够继续下去。

至于长篇文章的写作，ChatGPT 所做的实际上是模仿同类的文章，不过会用用户提供的新的信息取代旧的信息。比如我们前面提到的那两位深度使用者，他们就是在不断提供新的信息，引导 ChatGPT 的写作方向。ChatGPT 写的作文其实没有什么营养，内容只是比较巧妙的重复。不过客观地讲，今天大部分中学生写的作文其实质量都不高，也不过是把范文抄来抄去，甚至写得还远不如 ChatGPT。毕竟，模仿和抄袭，人是做不过机器的。

就在我修改这门小课的时候，阿里巴巴也推出了一个类似 ChatGPT 的产品让我试用。于是我发现了一个特点，写一些简单的邮件，ChatGPT 的行文是美国式的描述，像是从英文翻译过去的，而阿里巴巴的是中国式的口吻。它们的语言质量我不做评论，但这个现象说明一个问题：你输入给它什么训练数据，它就给你写出什么样的文章。

下节预告：

你可能马上会想到一句很有名的话，叫「垃圾输入、垃圾输出」，这正是 ChatGPT、甚至今天机器学习方法固有的问题。

下一讲，我们来讨论这个问题。