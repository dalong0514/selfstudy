## 0601. ChatGPT 有哪些固有缺陷？

上一讲，我们了解了 ChatGPT 工作的原理，这一讲，我们来看看它所存在的问题。

我们会看到，ChatGPT 回答问题、写文章，有的时候很靠谱，有的时候又在胡说八道，这些问题能不能解决呢？

我先说结论：有些问题是可以通过改进解决的，有些则是 ChatGPT 固有的问题，甚至是今天机器学习方法固有的问题，要解决是很难的。

### 6.1 哪些问题能够改进

我们先来看看哪些问题是通过改进能够解决的，这是我们将来能够期望的。

首先，有些错误是因为信息量不够所造成的，这部分错误将来 ChatGPT 是可以避免的。人类解决的问题会越来越多，互联网上的优质信息也会越来越多。这一点很容易理解，我们就不展开谈了。

不过，这部分问题解决并不会很快，因为人类创造新知识的速度没有那么快，网络上的文字虽然多，但是每年增加的新知并不多。

ChatGPT 还有一个问题，就是它创作出来的作文段落读起来显得乏味、不生动，用美国一些高中语文老师的话说，就是味同嚼蜡。这些问题也可以解决，毕竟语言模型的技术还在进步。

实事求是地讲，今天 ChatGPT 产生的段落，和十年前我在 Google 生产的段落相比，通顺程度已经好了很多。打个比方，十年前生成的段落相当于初中生的作文，今天有点像初三或者高一学生的水平了。

这样水平的写作已经有了一定的应用价值。比如中国学生申请美国的大学，英语作文一定是写不过美国高中生的，只要你给它规定清楚写什么，用 ChatGPT 写可能比自己写得还通顺些。考虑到自然语言处理技术进步比较快，ChatGPT 的写作可能很快会超过高中生的平均水平。

另一个有希望解决的问题是，今天的 ChatGPT 还离不开后面大量的人工，而且运行的成本特别高。这个问题，随着算法的改进是可以慢慢解决的。举个例子，战胜李世石的那一版 AlphaGo，消耗的能量是一栋十几层办公大楼的耗电量，但是两年后战胜柯洁的那个版本，耗电量就减少了两三个数量级。

### 6.2 垃圾输入，垃圾输出

听到这里，你是不是感觉很乐观？但是，ChatGPT 还有很多问题，它们属于今天完全倚仗数据的机器学习的固有问题。这些问题不是通过增加模型的规模，利用深度学习提高模型的精度，或者挖掘更深的语言知识就能解决的。

对于这一点，很多人不以为然，觉得技术进步了，语言模型和机器学习本身的问题都能解决。不仅外行人这么想，甚至很多从业者也这么想。这让我想起已故的著名物理学家张首晟教授讲的一句话：很多人做了多年研究，结果把物理学的第一性原理都忘了。

在信息领域，信息论所划定的边界，是不可逾越的，就如同光速和绝对零度不可逾越一样。语言模型进化到今天，虽然进步了很多，但依然是一个利用已有的信息预测其他信息的模型，这个性质没有改变。因此，如果你不给它提供足够多的信息，它就无法做事情。这就如同你不给汽车提供燃料，它就无法走一样。

从原理上讲，今天几乎所有的人工智能产品都是复读机。先要有各种知识和信息，ChatGPT 才能工作。你给它提供高质量的数据，它就会产生一个高质量的语言模型，然后给出高质量的答案，写出高质量的文章。相反，你用垃圾数据训练它，它就输出垃圾。

什么叫垃圾数据呢？其中一部分就是噪音。上个世纪 90 年代，美国语音识别的科学家就发现，如果在安静环境下，用麦克风录制语音进行识别，错误率很容易降到 10% 以下。但是，如果是电话录音，由于电话有噪音，错误率会超过 30%。对于语音来讲，噪音就是一种垃圾。

在 2000 年前后，我在约翰霍普金斯大学和 Google 都做过这样一些实验，把训练语言模型的文本混入一些噪音，比如我们在文本里混入一些错别字，或者把一些字的次序交换一下，语言模型的质量会大大下降。

另外，噪音越多，质量下降的速度越快。比如我们用语言模型进行从拼音到汉字的转化，当噪音在 1%、2% 时，不太会影响转换的准确率；噪音到了 5% 时，错误率就会明显上升两三倍；当噪音达到 10% 的时候，错误率就会上升十多倍。如果噪音更多，语言模型就不起作用，产生的结果就是随机的了。

使用过 ChatGPT 的朋友会有这样一个体会，当你和它谈论一些有争议的话题时，它给出的回答可能非常不靠谱，前后自相矛盾，完全没有逻辑，甚至和话题不沾边。为什么会这样？因为在网上关于那个话题的讨论，本身就非常不靠谱，而 ChatGPT 学习了那些内容后，会将不靠谱的表现放大。这就是机器学习中很有名的那句话：垃圾输入，垃圾输出。

有没有什么解法呢？通常只有两个做法。

第一个做法是在噪音不太高时，增加训练的数据量，这个做法是有效的，但是需要多用好几倍的训练数据。比如语料库里混有了 5% 的噪音，将语料库的规模增加十倍，可能可以弥补这个不足。

第二个做法是在能够找到噪音来源时，过滤掉噪音。比如你在开车时使用苹果的 Siri 服务，其实引擎的声音会进入到麦克风，但那是固定频率的噪音，Siri 会将它过滤掉。再比如，我们发现某个网站的内容一直不靠谱，就将相应的内容删除。

但是，相当一部分噪音是随机产生的，我们今天还是无能为力。这是今天机器学习的一大问题。

除了噪音，今天机器学习还有一个问题是：它所依赖的正反馈，有时会将它引向歧途。

什么是正反馈呢？比如你在短视频网站上看了几个 NBA 篮球的短视频，系统就得到一些反馈，觉得你会喜欢类似的视频，然后就调整了特别针对你的推荐模型，多给你推荐 NBA 的节目。这就是系统自适应的正反馈。这种做法通常让使用者觉得非常贴心，越用越好用。很多人对短视频和无厘头的推文上瘾，就是这个原因。

但是，自适应的正反馈是把双刃剑，如果有人刻意引导 ChatGPT 犯错误，这种正反馈机制会导致它错误百出。事实上，很多使用者已经发现，ChatGPT 在回答很多问题时已经被人「教坏了」。比如，有的用户可能是处于开玩笑的目的，给它提供了很多做违法事情的信息，以及很多仇恨的信息，结果它会教人如何杀人，如何做炸弹，并且时不时说出种族歧视的言论。

和过去不同的是，今天我们处在一个信息过载的时代。今天人们发愁的不是无法获得信息，而是信息太多，自己看不过来。很多信息还彼此矛盾，让人们无所适从，更不用说那些毫无营养、让人上瘾的视频和推文了。

面对这样一个信息过载、信息和噪音难以分辨的世界，我们自己判断一条消息真假的成本都很高，ChatGPT 本身更是无能为力。我在前面讲了，它像是一个厨师，你给它有营养的食材，它有可能做出一道既有营养、味道也还不错的菜。但你给它垃圾数据进行训练，它输出的也只能是垃圾。这是今天机器学习普遍的问题。

### 6.3 人工干预的边界

可能有人会想，是否可以进行人工干预，手工过滤掉那些垃圾输入？或者对于敏感的问题干脆不做回答呢？

事实上，ChatGPT 背后是有人工干预的，那些带有仇恨的言论已经被删除了。但是，这个本身有一万亿参数的模型很难进行人工调整。这个巨大的语言模型就像是一个黑盒子，你无法搞清楚里面那些模型参数的含义。

事实上，今天的语言模型和上个世纪早期的语言模型已经有很大的不同了。早期的语言模型比较简单，通常是直接把上下文的概率存在里面。今天的语言模型，存储的是人工神经网络的参数，从那些参数，你完全看不出它们和概率的大小直接的关系。换句话说，你很难通过人为调高或者调低一些参数来控制 ChatGPT 的输出结果。

人工干预还有一个很大的隐患，就是把人主观的好恶加进了一个原本应该客观的语言模型中，这可能导致更大的不公平。

在此之前，推特的人工干预就造成了很坏的影响。在 2020 年美国总统大选期间，推特根据自己的好恶，封掉了它不喜欢的特朗普的账号。这显然是在滥用权力。随后，马斯克认定推特的做法违反了言论自由的原则，收购了推特并且赶走了全部的管理层。在完全控制了推特之后，马斯克来了一个 180 度的大转弯，一方面恢复了特朗普的账号，另一方面封掉很多媒体的账号。

今天的 ChatGPT 已经是一家平台公司了，如果里面的人随意根据自己的好恶选择训练数据，控制结果。这个危害可能比操控推特更大。

下节预告：

如果你是 ChatGPT 的粉丝，听完这一讲可能觉得我在泼凉水。其实我们不是针对这款软件，而是要指出当前这种基于机器学习的智能系统普遍存在的问题，提醒你注意。

在 ChatGPT 的热度过去之后，很多企业和个人都对它进行了全面的测试。大家比较一致的看法是，对于一些有明确答案的问题，它还是能回答得很好的。但是对于缺乏有用信息的问题，它给出的答案相当随意，基本上只能是重复车轱辘话，没有太多的价值。根本原因就是这一讲解释的：垃圾输入，垃圾输出。

下一讲，我们来看看造一个 ChatGPT 需要多少资源，以及它会受到怎样的限制。