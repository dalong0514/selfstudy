## 0701. ChatGPT 需要什么资源？

ChatGPT 走红之后，国内很多媒体和人一下子慌了神。

各种媒体，特别是自媒体上，铺天盖地都是诸如「为什么中国没有诞生 ChatGPT」「美国将要爆发一场新的技术革命」之类的文章。很多研究机构和企业都宣称，我们马上就要推出自己的 ChatGPT。还有很多人觉得，ChatGPT 给自己带来了新机会。

其实，这样全民大炼 ChatGPT，既不必要，也不可能。

这是为什么呢？不必要性，我们后面会慢慢展开讲。这一讲，先来谈谈不可能性。

我们前面讲到，ChatGPT 的核心是语言模型，而语言模型需要用大量的数据来训练，有了数据后，还需要强大的算力支持，然后还需要有足够高水平的并行计算和机器学习的算法支持。数据、算力、算法三道坎，只有极少的互联网大厂能够越过去。

接下来，我们就从这三个方面分别来看一下。

### 7.1 限制一：数据

首先说第一个限制，数据。

早在 2012 年，我在 Google 训练自动问答的模型时，就用到了互联网上能找到的几乎所有高质量的数据。今天的 ChatGPT 所用的数据更多，特别是自媒体上的数据和社交网络上的数据。

这个数据量是极其庞大的。不是每个企业都能得到所有的这些数据。

### 7.2 限制二：算力

而且，就算一家企业今天能得到这些数据，也没有能力存储和处理这些数据，因为处理它们的计算量太大。这就要说到训练语言模型的第二个限制 —— 算力。

前面讲过，早期的算法非常简单，就是对文本做一些统计，然后利用一些简单的数学公式，计算出一个词在不同上下文条件下的概率，事先保存下来。使用的时候，根据这些概率，计算出各种语言现象的概率，比如一句话的概率，或者某个词是名词的概率，或者给定一个问题，某句话是它的答案的概率。

这是截止到上世纪 90 年代的方法。当时，即使做这样简单的统计，所消耗的算力也是惊人的，只有 IBM 这类的大公司才能办到。

但是很快，贾里尼克等人发现，如果只是对文本进行简单的统计，再多的数据也无法覆盖所有的语言现象，而且对于很多语言现象，特别是之前没有见过的语言现象的概率，语言模型估计得都很不准确。于是，人们开始考虑深度挖掘语言中所蕴含的信息，包括语法信息和语义信息。这就需要更大的计算量了。

具体计算量需要增加多少呢？100 万倍到 1 亿倍是一个合理的估计。

很多人觉得，摩尔定律可以让计算的成本不断下降，过去做不成的事情今天能做成了。其实，随着时间的推移，人们对人工智能的要求也在不断提高，需要的算力也在不断增加。算力，永远是一个瓶颈。

上个世纪 90 年代末，贾里尼克为我们申请到了一笔美国国家科学基金会（NSF）的基金，让我们有经费建立复杂的语言模型。与此同时，全世界机器学习领域在算法上也有所突破，能够训练更准确的语言模型。此外，贾里尼克还从美国国防部获得了一笔巨额的经费，让我们能够装备上百台超级服务器。

在这些设备的帮助下，我们得以进行其他研究单位没有条件做的语言模型的改进工作。

1998 年，我参加 IEEE，也就是电气与电子工程师协会的一个重要的学术会议。在我前面做报告的是一个德国著名的科学家，他给出的结论是，更好的机器学习算法似乎对改进语言模型无效；而我接下来的报告则否定了他的结论。我们的研究表明，更好的机器学习的算法很有效，但是需要训练很长的时间，直到模型真正收敛。我们当时能调动的算力大约是他的 100 倍，因此，他做不了的事情我们能做。

但是，即使我们当时有学术界最强的算力，很多想做的事情也还是做不到。当我试图把各种语法信息加入到语言模型中的时候就发现，以我们的算力来做这件事，十年可能也算不完，我也根本无法毕业。

当时，我们看到一位 IBM 的科学家做了类似的事情，并且发表了一篇文章，于是就请他来作报告，想了解一下他是怎么做的。其实，他真正了不起的地方不是在方法上有什么独到之处，而是他写了一个程序，将整个 IBM 沃森实验里闲置的服务器都利用起来，然后计算了整整一年。这个咱们前面也讲到过。

后来，我的导师们希望我超过这位科学家，于是我花了半年的时间从数学上改进算法，将训练的计算量降低了 200 倍，然后在 40 台超级服务器上运行三个月，终于训练出一个当时世界上最好的语言模型。如果没有这 200 倍的改进，我需要计算 50 年。

举这个例子，不是说我有多厉害，而是让你明白，算力对机器学习的重要性。你看，即便当时大幅改进了算法，减少了计算量，仍然需要 40 台超级服务器上运行三个月。

今天 ChatGPT 所采用的语言模型，从参数的数量上讲，是我当时训练的模型的 10 万倍，比我在 Google 构建的模型也大了 1000 倍。开发它所需要的算力，可不只是我在约翰霍普金斯时的 10 万倍，而可能是上百亿倍。

今天，开发 GPT 语言模型的公司 OpenAI 没有讲它使用了多少计算资源，但是它暗示，硬件成本超过一亿美元。如果我们按照 1000 美元一个 GPU 计算，它使用了 10 万个 GPU。以 32 位运算为准，它能提供超 100PFLOPS 的算力，也就是每秒 10 亿亿次运算以上。这是整个特斯拉云计算集群计算能力的两倍，是 Google 最大的数据中心算力的 1/3，是阿里云最大的数据中心算力的 1/4。

这还只是在它开发 GPT-3 时的花费。到了升级为 GPT-4 时，需要的计算资源就更多了，用到了微软云计算大量的计算资源。

即便如此，ChatGPT 最近还关闭了它的付费用户的注册，因为为付费用户提供更优质的服务太消耗计算资源了。

因此，如果你也想做一个 ChatGPT，先要考虑一下自己的算力够不够。很多人一看到出了个新技术，就觉得机会来了，但是绝大部分时候，那些机会和大家无关，因为大家没有基本的能力实现它。

### 7.3 限制三：算法

那么，如果我们举全国之力建几个超级计算中心，是不是很容易超过 ChatGPT 呢？

如果只看算力，似乎可以。但其实，算力也分「聪明的算力」和「笨的算力」，对于人工智能问题，只有前一种 —— 聪明的算力 —— 才有意义。

所谓「聪明的算力」，就是说，光有算力还不够，你还得有配套的算法把算力的作用发挥到极致。如果没有配套的优质算法，算力就只能事倍功半。这就是笨的算力。

这就说到了训练语言模型的第三个限制 —— 算法。

2000 年，我在约翰霍普金斯构建语言模型时，一次能调动 40 台服务器同时工作。当时还没有云计算一说，我靠的是加州大学伯克利分校提供的一个并行计算的工具。这让我不需要手工监控每一台服务器的工作，而且能自动分配任务到空闲的服务器上。后来有了云计算，这件事就容易了很多。

但是今天，训练语言模型用到的机器学习算法要复杂很多。

2010 年的时候，Google 推出了一个基于云计算平台深度学习的工具 ——Google 大脑。采用这个工具，语言模型的效果可以大幅提升，在其它条件不变的情况下，语音识别和机器翻译的相对错误率可以降低 10% 以上。

今天，深度学习的基本算法和支持它的基础架构，已经是智能化的数据中心，有些地方也称之为「智算平台」的标配。

如果只是用一个由处理器，包括 GPU，堆砌起来的数据中心训练语言模型，得到的结果会差很多。

也就是说，除了算力之外，基础的自然语言处理技术，也就是算法，也是实现 ChatGPT 这些产品必不可少的条件。

最直观的例子，你想让计算机回答问题，至少要让它懂得问题。你要想让它从上千亿的文本中知道哪些可能是答案，需要做到问题和答案的匹配。这个工作不是一年半载就能完成的。

今天，很多机器学习的算法已经开源了，有些应用已经有公司和研究机构投入科研力量开发过，比如基本的图形识别和语音识别技术，但是深度的自然语言理解其实还不属于这个范畴。在这些领域所具有的技术积累，其实也是一种资源。

因此，一些长期在这个领域有投入的公司，比如 Google、微软，中国的百度、字节跳动等，其实是有一定的技术积累的，它们可能通过对现有的技术的集成，在较短的时间里，做出类似 ChatGPT 的产品。

但是，对于那些完全不具备应有的技术资源的人来说，跟风 ChatGPT，要么是无知者无畏，要么是炒作。

我们前面讲了，像 GPT 这样的语言模型技术，除了处理自然语言，还可以在很多领域找到应用场景，比如在生物医疗领域、在视频编辑方面，这就需要具有相应的算法，这里我们就不一一列举了。因此，没有足够的算法资源，也不建议大家都去打造 ChatGPT。

下节预告：

好，这一讲，我们说了训练语言模型的三个限制，分别是数据、算力和算法。这三道坎，只有极少的互联网大厂才能越过去。

其实，即便跨过了三道坎，做出了比 ChatGPT 更好的产品，它也不可能像很多人想象的那样无所不能。因为 ChatGPT，乃至人工智能，本身就是有能力边界的，并不能解决所有问题。
