# 0203生命3.0三AI的文明使命

> 日课009｜
万维钢 2017-09-27

多年以后，你面对养老院的 AI 护士，被抢走了酒杯和遥控器，勒令上床休息的时候，可能会想起今天这篇日课。

科幻小说作家畅想未来经常会犯两种错误。一种错误是他没有充分考虑 AI，还以为都是人在主导一切。另一种错误是他误判了 AI。像《西部世界》这样的电视剧里，机器人动不动就活了 —— 有人的意识，但是智力水平居然并不高于人，有时候还挺笨的，比如《终结者》里的机器人纯粹是个怪兽。

真实情况是让 AI 获得意识非常非常困难，但是让 AI 的智能超过人则比较容易。写小说最难写的是比作家自己聪明的人，科幻作家很难想象超级 AI 是一种什么样的存在。

但是科学家们已经想了很多很多。咱们继续说泰格马克的《生命3.0》。未来的人类文明会使用什么样的手段去获取能源，用什么样的方式生活？美国人爱说一句话叫「Sky is the limit」，翻译成中文大概叫「只有天空是尽头」—— 如果你考虑到超级 AI 的存在，那天空根本就不是尽头。物理学的限制，才是尽头。

泰格马克像写论文一样做了大量的调研，等于是把 AI 影响下未来所有的可能性都列举了一遍。科幻小说作家完全可以把这本书当做「未来穿越指南」—— 他们会发现这些科学家的设想比小说家的幻想还奔放。

咱们就从近期到远期，展望一下 AI 背景下的人类文明。我们说过真实世界并不一定是美好的，你未必会喜欢未来的样子。 

## 1. AI 听命于人类

AI 驾驶汽车和 AI 参与医疗这些技术现在已经几乎成熟了，当前的主要问题是解决可靠性。

从 2000 年到 2013 年，美国医疗手术机器人事故导致的病人死亡一共有 144 例，受伤 1391 例，这个数字听起来挺多，但这可是两百万个机器人手术才有这么多事故。对比之下，美国每年因为人类医生的错误而导致的死亡超过十万例。自动驾驶汽车也是这样。绝大多数交通事故都是因为人的错误。有人估计，如果把路面上所有汽车都交给 AI 开，交通事故至少能下降 90%。

但问题就在于剩下的这 10% 算谁的。到底应该让车主负责，让汽车厂商负责，还是让开车的 AI 负责？考虑到这种责任风险，厂商必须把 AI 事故率降低到极限，才敢推向市场。

说到法律问题，也许我们应该把法官的判决权交给 AI。人类法官就算不腐败也不一定能确保公正，我们知道一个著名的研究发现，以色列法官仅仅因为自己感到有点饿了就会草率地否决犯人的保释资格。可是如果真让 AI 主导法律，又有别的道德问题。黑人的犯罪率更高，如果从概率论角度，出了事儿也许就应该把黑人作为首要怀疑对象 —— 可是如果 AI 真这么干，那是不是系统性的种族歧视呢？

像这样的道德困境，最严重的是军事应用。以前科幻小说作家阿西莫夫搞了个「机器人三定律」，第一条就是机器人不得伤害人类 —— 他到底想过没有，很多关键的技术进步都是军事应用主导的。人们早就已经把 AI 用在了武器上，而且考虑到人类指挥官反应慢，也许将来会授予 AI 直接开火的权力。

有人说应该设定国际公约，禁止把 AI 用于武器，而任何一个懂国际政治的人都会告诉他这禁止不了：就算大国能克制自己，小国也会偷偷搞。

还有我们说过多次的，AI 可能让很多人失业。所以你看，AI 兴起的世界中其实有很多新烦恼。

但那个世界毕竟还是人类说了算。 

## 2. AI 与人类共治天下

再往远考虑一点，恐怕就不仅仅是 AI 为人类服务了，人和 AI 之间会有主导权的斗争。

比如说，假设现在有一套遍布全国的监控系统，由 AI 统一管理。它给每个人身上带一个身份识别设备，它知道每个人在干什么，它甚至能根据一个人的性格和情绪预测这个人将会干什么。你说这套系统会不会变成独裁者的统治工具？

可能由不得你。历史规律是谁只要有这样的能力，就会使用这样的能力。一开始可能都是善意的，系统开启之后所有人的生活都更安全了。但掌握系统的人会越来越不耐烦，他会觉得要想让世界更好就需要更多的控制。甚至再进一步，也许 AI 索性接管控制权，干脆让人类就不要插手……

Google 的拉里·佩奇这些人总是鼓吹 AI 可以人类和睦相处。但 AI 为什么要跟你和睦相处呢？如果你活也干不好，判断还总出错，AI 为什么不索性自己干？

人类不就想要幸福吗？全听我的你们更幸福。也许人类会被 AI 当成宠物，你的任务就是「幸福」。可是当宠物未必幸福。也许你想多生几个孩子，AI 说现在资源不够，或者你的基因不行，别生了。也许你想参政议政，AI 说你不懂别瞎说了。

你可能说人类应该设定好，不给 AI 主导权。但泰格马克说这其实很难。只要 AI 有了足够的能力，它就会使用自己的办法摆脱人类的控制 —— 别忘了这可是比人类聪明得多的智能！

专家们开了一些脑洞。比如说，能不能让 AI 扮演上帝的角色，关键时候出来帮助一下人类，平时就把自己隐藏起来。还有没有可能用 AI 去监管 AI，只要最强的 AI 忠于人类，其余所有的 AI 都会忠于人类。又或者我们能不能控制 AI 的进步速度，考虑到让 AI 做大之后的危险，能不能全面禁止技术进步，就像《1984》小说里面写的那样，所有人互相监督，谁也不许再搞科技创新？再或者说，能不能我们干脆来个技术大倒退，把 AI 技术封存，所有人过像现在美国的阿米什人那样的生活？

所有这些全面限制 AI 的手段都不太现实。除非你有一个统一了全世界的强权，否则就算你们国家限制 AI，别的国家也会发展 AI。越是弱国就越可能把灵魂卖给 AI，换取自身的强大。

## 3. AI 主导人类文明

如果我们考虑更远的尺度，比如说一万年以后，你几乎肯定那时候 AI 已经完全超越了人类。人类是否还有必要存在，都是个问题。

人是个效率很低的生物。用爱因斯坦的质能方程 E=mc^2，看各种获取能量的方式相当于消耗了多少质量来算的话，我们吃糖获得的能量消耗效率，相当于 0.00000001%。烧煤的效率相当于吃糖的三倍。汽油相当于吃糖的五倍。这都是非常低效的能源。

如果你使用裂变反应的核能，效率一下子达到 0.08%。用核聚变，效率能达到 0.7%。可是如果将来的 AI 能做到用黑洞发电，根据物理学家的理论，效率可以达到 20% 以上，甚至达到 90%。

我们现在说的节能减排，对未来的 AI 来说可能就是个笑话。就拿太阳能来说，只要建一个占地相当于撒哈拉沙漠 0.5% 的面积的太阳能发电站，就足以获得当前人类所需的所有能源。

而未来 AI 要用太阳能，可不是这么用的。咱们以前介绍过的物理学家弗里曼·戴森，设想了一个叫做「戴森球」的系统，告诉我们将来 AI 会怎么做 ——

AI 会在外层空间造一个巨大的球体，把太阳整个给包围起来。然后我们人类文明就可以生活在这个球体上，面积绝对够用怎么折腾都可以 。这么一来，我们就把所有的太阳光都用上了，一点都不浪费。只要球体足够大，阳光并不会很刺眼，球的内侧永远都是白天 —— 而如果你想看星空，直接下楼去球的外侧看。建设这么一个球需要的资源从哪里来呢？也许 AI 可以先把木星拆了。

有了这种规模的超级能源，AI 的目标就是向整个宇宙殖民。现在人类最快的太空探测器速度只有光速的 0.1%，而物理学家设想的一种激光推进的飞船，理论速度可以是光速的一半。

AI 殖民不需要载人。AI 可以带着人类的 DNA 信息，用 20 年的时间飞到十光年以外的一个星球，在那里建设十年，用 DNA 信息现场组装出人类来。然后 AI 可以从那里出发再往前推进。 如果这么算的话，地球文明几乎就是以光速去殖民整个宇宙，文明扩散的唯一的限制是暗能量导致的宇宙膨胀！ 

这个星辰大海的前景只有两个问题。

第一个问题是，这一切可能都跟人类无关了。如果 AI 主导一切，人的位置在哪里？人存在的意义在哪里？

有人设想，我们应该想开一点，干脆就把 AI 当成自己的孩子吧。作为父母我们没什么能耐，但是我们的孩子厉害，这难道不也是一种安慰吗？没准 AI 还会哄哄我们，说我永远是你的孩子，我继承你的姓氏，我代表你去征服星辰大海……那样你是不是也有点自豪感。毕竟是地球文明的种子播向了整个宇宙。

如果你说这不行，我不想被 AI 代表，我不想征服十光年以外的地方，我就想老老实实待在地球过日子……那你可能就对地球文明太不负责任了。因为有些日子不是你想过就能过的，你会遇到各种灾难的挑战。

一百年之内，我们迫在眉睫的灾难就是核战争。一千年之内，地球很有可能会出现灾难性环境。在一万年的范围内，我们很有可能遭到小行星的撞击，把地球文明毁灭一半。在十万年内，我们很有可能遭遇到超级火山的爆发，也把地球文明毁灭一半。在一百万年或一千万年的尺度上，我们很有可能遭遇一次巨大的小行星撞击，把整个地球都给毁灭掉。

就算人类文明躲过了这些，你还要知道太阳的寿命是有限的。十亿年之后，太阳会变得特别特别热；一百亿年之后，太阳会变成红巨星，一直膨胀到把地球给淹没掉。

没有 AI，人类文明就不会有未来。把未来交给 AI，我们不但可以躲过地球上的各种灾难，甚至有可能整体移民外星球，也许文明的寿命可以和宇宙相等 —— 只不过那时候也许就不能叫「人类」文明了。

第二个问题则是，AI 愿意去做这一切吗？AI 会不会有自己独立的意志，它还会不会在乎我们这个文明？ 这些问题咱们下次再说。

泰格马克创办「未来生命研究院」，就是为了在 AI 做大之前，让人类把这些问题想清楚。

读读这本书，再想想咱们国内某些专家学者，不知道什么叫算法，不知道什么叫图灵机，对 AI 一窍不通，居然还要指点未来社会怎么前进，这不胡闹吗？





