# 0202. 生命3.0二I与AI

> 万维钢 日课008 2017-09-26

人工智能到底能不能完全模拟人的智能 —— 也就是今天标题里的 AI 到底能不能等于 I，是一个无比重要，而又充满争议的问题 。说人脑没什么特别的，无非也是一堆原子组成的东西，那我们就完全可以用另一堆原子模拟这堆原子，电脑总有一天能取代人脑……这是非常轻率、没有什么营养的说法。

想要合理推测，你得知道人脑有多厉害，更得知道现在的电脑都是什么原理，然后你还得猜测人脑是不是基于同样的原理。

现在全世界最厉害的超级计算机是咱们中国国产的，叫神威·太湖之光——

它每秒能进行 10^17 次浮点运算，需要一个占地 1000 平方米的专用机房，它的总造价大概是人民币 20 亿元。

这种水平计算机的存在对脑科学家是个好消息，因为想要模拟人脑中全部的神经元的行为，你就至少需要一台「神威·太湖之光」。而这还仅仅是神经元水平的模拟，有的科学家认为模拟人脑必须达到分子水平，那就在可以预见的几十年里恐怕不管什么计算机都无能为力了。

就算神经元水平已经足够，你真的能用「神威·太湖之光」完全模拟一个人的大脑，能取代人的工作，你也未必想这么做，因为成本实在太高了。这种超级计算机不但造价高还费电，你直接雇几个工人才多少钱。

所以正确的策略不是模拟一个人脑，而是模拟人的「智能」。我们今天要说的，就是现在的 AI 是通过很可能完全不同于人脑的原理，在相当程度上实现、而且还超过了人的智能。我们还是说泰格马克的《生命3.0》。

什么叫「智能」呢？泰格马克给了一个比较笼统的定义：智能就是完成一个复杂目的的能力。当然，你可以进一步追问什么叫复杂，这都是科学家也说不清道不明的概念……不过这个意思你显然理解。反正能随机应变地完成一些复杂的任务，就可以叫做智能了。

想要实现智能，AI 大概只需要三种能力：存储信息、计算，和自我学习。而至于说人还有意识、主观的情感体验这些，也许重要也许不重要，咱们过几天再讨论。

我们来看看计算机和人在这三个基本能力上的对比。 

## 1. 存储能力

人的大脑以生物方式能够存储的总容量，大约是100TB。一部高清电影压缩之后大约 2G，那么这 100TB 可以存储五万个电影 —— 这可是包括其中所有的细节。所以我们想想，大脑一般用用是不怕不够用的。这 100TB 相当于是计算机的硬盘，我们所有的记忆、所有的技能都存储在其中。

跟计算机差不多，大脑考虑问题的时候不能总是从硬盘读取信息，还有一个快速读写的机制。在大脑中，这就是以神经元电信号的形式存储的信息，这个容量就小得多了，大约是 10G，正好是现在一般水平个人电脑内存的大小。

对今天的计算机来说，100TB 的硬盘和 10G 的内存都不算什么，而且随着技术进步存储的容量越来越大，价格越来越低。

你可以说计算机存储信息的方法和人存储信息的方法是不一样的 —— 计算机存储是按照地址索引，就好比找一本书，先记住这本书所在位置，再去寻找。而人存储信息是用神经网络，先想到大概的内容，然后一点一点回忆相关的细节。不过，人脑这种存储信息的方式并没有什么神秘的，已经有人证明，如果用神经网络的方法存储信息，每 1000 个神经元可以存储 138 条信息。

总之在存储方面，计算机是肯定没问题。 

## 2. 计算能力

对计算机科学家来说，人生中最值得赞叹的时刻肯定不是目睹 AlphaGo 打败柯洁。早在很久很久以前，自从「计算机」这个概念诞生那一天开始，甚至还没有一个实用化的计算机的时候，科学家就已经知道计算机可以下好围棋了 —— 悬念仅仅是需要多少时间。

这个赞叹必须属于祖师爷阿兰·图灵。

2014 年有个电影叫《模仿游戏》（The Imitation Game），讲图灵怎么用自己发明的计算机破译德军密码，从而帮着打赢了二战的事迹。后世的人也许会说相对于图灵在计算机科学上的伟大贡献而言，打赢二战只是一件小事儿。

这个关键概念，叫做「图灵机」。图灵设想了这么一种简单的计算机，它可以读取信息，然后按照一定的规则操作，修改和输出新信息。它的结构并不复杂，你可以把所有信息、包括程序在内，都存放在一条纸带上，计算机就操作这条纸带 ——  

你用的个人电脑、手机、包括以前那种特别土的计算机，都是图灵机。它们的基本原理是完全一样的，几十年来所有的技术进步仅仅是让存储能力更强，运算速度更快而已。

这就是说，计算并不神秘。凡是能用算法说清楚的问题，都可以用计算机实现。理论上这些都解决了，哪怕最简单的计算机都能完成所有计算，剩下的限制都是物理上的：你需要给它足够的电力让它运算，以及提供足够大的存储空间。 

## 3. 学习能力

近几年之所以出现了人工智能的大跃进，大概主要得归功于所谓「深度学习」的技术进步。深度学习其实就是过去计算机科学家们早就在用的所谓「神经网络」算法，只不过算法上有些改进，最重要的是硬件水平和数据量大大提升了。

请注意，这里说的「神经网络」，并不是直接做一个像人脑的神经网络那样的计算机 —— 我们用的还是图灵机，神经网络只是一个模拟算法。

人脑学习新技能，是发生在神经元这个层面的。因为练习一个动作而经常被一起触发的神经元，最后就会长在一起，整个网络结构长好了，就相当于一个技能长在了你的大脑之中。

并不需要多么复杂的「神经元」就能实现这种功能。1989 年就有人证明，用最简单的神经网络反复训练，每次只要系统做对了就增加相关连接的权重，给足够多的时间最后它就能够做成任何事情。

从一张普通照片里识别各种物体也好、AlphaGo 下围棋也好，所有「深度学习」的基本原理都是这样的。神经网络算法，也是通用的。
 
泰格马克书中有很多技术细节，咱们时间不够只能忽略了，如果你感兴趣的话……这其实是一个无底洞。

那这些原理难道就足够模拟人的一切智能了吗？泰格马克对此持比较乐观的态度，但是我们知道有很多人不这么看。比如很多年以前英国物理学家罗杰·彭罗斯有本书叫《皇帝新脑》，在中国也很流行，那本书的观点就是人脑根本不是图灵机，基于图灵机的 AI 不可能真的具有人脑那样的智慧。

但是泰格马克的乐观也有道理。比如现在科学家已经知道，神经网络算法并不能解决所有问题，有些复杂的方程它根本解不了。但是泰格马克恰恰和他的学生写了一篇论文，说神经网络算法所「能解决的」那些简单方程，就已经足够对付真实世界了 —— 因为描写真实世界的物理定律也都是简单方程！

这其实是个很有意思的现象。基础物理定律的确都是简单方程，比如说最多只需要用到二阶导数。那为什么物理定律都是简单的数学方程呢？这个问题其实很有意思，咱们以后找机会再说。

今天这个道理是， 也许图灵机和神经网络算法不能完全取代人脑，但是对于真实世界需要的智能来说，它们可能就已经够用了。

咱们把存储、计算和学习这三点综合起来，你发现其中所有的底层原理都是逻辑意义上的。也就是说，这些原理跟把信息存储于什么介质中、用什么东西来计算无关。AI 的硬件，可以随便升级。这就是生命 3.0。

硬件能升级到什么程度呢？有人说摩尔定律快要到极限了，泰格马克说这根本不叫事儿。如果你不局限于用基于硅的芯片，那计算能力最终只受到物理学的限制。而物理学的限制是，人类理论上可能拥有的计算能力是今天的 10^33 倍 —— 哪怕我们每隔几年就把计算能力增加一倍，也需要 100 年的时间才能达到真正的物理极限。

硬件持续升级之后的 AI 到底意味着什么，是我们下一期的内容。

